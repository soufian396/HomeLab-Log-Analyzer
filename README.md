# ğŸ§  HomeLab Log Analyzer â€” Zero-Noise, AI-Powered Daily Summaries  
A fully-local, fully-automated system that turns your chaotic Docker logs into clean, structured, actionable data... every night... powered by your own local LLM.

No cloud.  
No API fees.  
No bullshit.  
Just *pure self-hosted magic* âœ¨

---

## ğŸš€ What This Project Does
This tool automatically:

- Collects logs from all (or selected) Docker containers  
- Removes noise, health-checks, pings, spam, repeat lines  
- Feeds the cleaned logs into **your local LLM** via LM Studio  
- Generates a structured **JSON summary** of critical issues, warnings, successes, anomalies, and recommendations  
- Sends you a **daily email report** via N8N  
- Runs 100% offline, on your own hardware

Think of it as your homelabâ€™s private Network God.

---

## ğŸ—ï¸ How It Works
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   N8N       â”‚â”€â”€â”€â”€â”€â–¶â”‚  FastAPI App   â”‚â”€â”€â”€â”€â”€â–¶â”‚  LM Studio   â”‚â”€â”€â”€â”€â”€â–¶â”‚  Email   â”‚
â”‚ (Scheduler) â”‚      â”‚ (Log Analyzer) â”‚      â”‚ (Local LLM)  â”‚      â”‚ Delivery â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      22:00               â†“                        â†“
                     Pull Logs             Summarize in JSON
                     Filter Noise          Return Clean Insights
```

**N8N triggers âœ Python FastAPI pulls & filters logs âœ LM Studio analyzes âœ N8N emails results.**

---

## âœ¨ Features at a Glance
- ğŸ³ **Docker log ingestion** (auto or per-container)
- ğŸ§¹ **Noise-filtering engine** (health checks, ping/pong, heartbeats, empty lines, etc.)
- ğŸ¤– **Local LLM analysis** (Qwen, Phi-3, Llama, anything LM Studio supports)
- ğŸ“¦ **Structured JSON output**
- ğŸ”” **Daily summary email**
- ğŸ”’ **100% private & offline**
- âš¡ **Lightweight â€” runs on a mini-PC or NUC**
- ğŸ” **Plug-and-play with N8N automation**

---

## ğŸ”§ Included in This Repository
- `log_analyzer.py` â€” Complete FastAPI backend  
- `n8n_workflow.json` â€” Drop-in N8N daily summary workflow  
- `docker-compose.yml` â€” One-command deployment  
- `Dockerfile` â€” Containerized API  
- `requirements.txt` â€” All dependencies  
- `quickstart.sh` â€” Setup in under 60 seconds  
- Comprehensive documentation:  
  - `docs/setup.md`  
  - `docs/models.md`  
  - `docs/troubleshooting.md`  
  - `docs/api.md`  
  - `docs/architecture.md`

---

## ğŸ§ª Example Output (JSON)
```json
{
  "critical_issues": ["Container 'db' restarted unexpectedly"],
  "warnings": ["High memory usage in 'plex'"],
  "successes": ["Backup completed in 'nextcloud'"],
  "recommendations": [
    "Investigate container restarts",
    "Review memory allocation"
  ],
  "container_status": {
    "db": "2 restarts detected",
    "plex": "Memory warning"
  },
  "overall_health": "degraded"
}
```

---

## ğŸ› ï¸ Installation
```bash
git clone https://github.com/WhiskeyCoder/homelab-log-analyzer
cd homelab-log-analyzer
python log_analyzer.py
```

---

## ğŸ¤– Why Local LLM?
Because itâ€™s:

- **Free** (no API bills)  
- **Fast** (sub-second inference on small models)  
- **Private** (no logs leaving your machine)  
- **Customizable** (your prompt, your workflow)

Models supported:  
- Qwen  
- Phi-3  
- Llama  
- Anything LM Studio can serve

---

## ğŸ“¨ Example Daily Email
- ğŸ”´ Critical Issues  
- âš ï¸ Warnings  
- âœ… Successes  
- ğŸ’¡ Recommendations  
- ğŸ“Š Per-container status  
- ğŸŸ¢ Overall homelab health

Beautiful HTML formatting included.

---

## ğŸ¤ Contribute
PRs welcome.  
New noise-filters, better prompts, new N8N flows â€” bring them on.

---

## ğŸ“œ License
MIT â€” use it, break it, improve it, ship it.

---

If your homelab produces logs, this tool turns them into clarity.

Give your servers a voice.  
